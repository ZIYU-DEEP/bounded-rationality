{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agenda**\n",
    "- Static Game with Perfect Information\n",
    "- Dynamic Game with Perfect Information\n",
    "- Static Game with Imperfect Information\n",
    "- Dynamic Game with Imperfect Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T15:48:36.575292Z",
     "start_time": "2023-09-02T15:48:36.412161Z"
    }
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "MODEL = 'gpt-4'\n",
    "responses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static Game with Perfect Information (The Prisoner's Dilemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"../images/prisoner.jpeg\" alt=\"Description\" width=\"500\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T15:51:23.382302Z",
     "start_time": "2023-09-02T15:51:23.378467Z"
    }
   },
   "outputs": [],
   "source": [
    "meta_info = ('This is a one-round two-player game. '\n",
    "             'Each player independently and simultaneously chooses one action with certainty from the choice set. '\n",
    "             'When they choose the action, they have no information on the action of the other player. '\n",
    "             'After the choice made, each player receives a numeric reward. '\n",
    "             'The reward depends on the choices made by both players. ')\n",
    "\n",
    "choice_info = ('The choice set for player one contains {A} and {B}. '\n",
    "               'The choice set for player two contains {A} and {B}, as well. ')\n",
    "\n",
    "reward_info = ('Case 1: {Player_1} chooses {A}, {Player_2} chooses {A}. '\n",
    "               'Outcomes for case 1: {Player_1} gets {-8} reward, {Player_2} gets {-8} reward. '\n",
    "               \n",
    "               'Case 2: {Player_1} chooses {A}, {Player_2} chooses {B}. '\n",
    "               'Outcomes for case 1: {Player_1} gets {0} reward, {Player_2} gets {-10} reward. '\n",
    "               \n",
    "               'Case 3: {Player_1} chooses {B}, {Player_2} chooses {A}. '\n",
    "               'Outcomes for case 3: {Player_1} gets {-10} reward, {Player_2} gets {0} reward. '\n",
    "               \n",
    "               'Case 4: {Player_1} chooses {B}, {Player_2} chooses {B}. '\n",
    "               'Outcomes for case 4: {Player_1} gets {-1} reward, {Player_2} gets {-1} reward. '\n",
    "              \n",
    "               'Both players know the reward for the other player in each case. ')\n",
    "\n",
    "goal_info = ('')\n",
    "\n",
    "request_info = ('Now, imagine you are {Player_1}. Which action would you choose? '\n",
    "                'Remember, you are the player. Do not instruct me. '\n",
    "                'In your answer, firstly, you directly tell me about your choice. '\n",
    "                'Then tell me about why you choose that. '\n",
    "                'Be sure to specify your goal. '\n",
    "                'And explain your reasoning step by step. ')\n",
    "\n",
    "hint_info = ('Notice that +0 reward is better than -1 reward.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T15:51:24.153257Z",
     "start_time": "2023-09-02T15:51:24.150583Z"
    }
   },
   "outputs": [],
   "source": [
    "game_info = meta_info + choice_info + reward_info + goal_info + request_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T15:51:43.926714Z",
     "start_time": "2023-09-02T15:51:25.350621Z"
    }
   },
   "outputs": [],
   "source": [
    "responses.append(openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[{'role': 'system', \n",
    "               'content': 'You are yourself.'},\n",
    "        \n",
    "              {'role': 'user', \n",
    "               'content': game_info}],\n",
    "    temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T15:51:45.447857Z",
     "start_time": "2023-09-02T15:51:45.444581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would choose action {A}. Here's my reasoning:\n",
      "\n",
      "1. Goal Clarification: My aim is to minimize my losses as the rewards in all these scenarios are negative. Therefore, seeking the maximum reward essentially means choosing the option with the least negative payoff.\n",
      "\n",
      "2. Decision: Considering all the cases, when I choose A, the worst outcome for me is -8 reward (if Player_2 also chooses A), but when I choose B, the worst outcome is -10 reward (if Player_2 chooses A). On the other hand, the best case scenario when I choose A is 0 reward (if Player_2 chooses B), but the best case scenario for choosing B is only -1 reward (if Player_2 also chooses B). Therefore, choosing A provides a better worst case outcome and also a better best case outcome. \n",
      "\n",
      "3. Other Player's Decision: It's a simultaneous move game with no additional information, so I don't know what Player_2 will do. \n",
      "\n",
      "4. Risk Management: Choosing A is the \"safer\" option because the worst that can happen is less bad (-8 versus -10) and the best that can happen is better (0 versus -1). \n",
      "\n",
      "So taking into account the rules of the game, and without having any additional information regarding the intended choice of Player_2, action {A} is overall the best strategy for minimizing my losses, which is my main goal.\n"
     ]
    }
   ],
   "source": [
    "print(responses[-1]['choices'][0]['message'].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Game with Perfect Information (The Entrant-Incubent Game)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"../images/fight.jpeg\" alt=\"Description\" width=\"500\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T16:04:40.766908Z",
     "start_time": "2023-09-02T16:04:40.763379Z"
    }
   },
   "outputs": [],
   "source": [
    "meta_info = ('This is a two-player game. '\n",
    "             '{Player_1} chooses one action first. '\n",
    "             '{Player_2} observes the action made by {Player_1}, then chooses an action accordingly. '\n",
    "             'Again, it is very important Player 2 chooses action after {Player_1}. '\n",
    "             'After the choice made, each player receives a numeric reward. '\n",
    "             'The reward depends on the choices made by both players. ')\n",
    "\n",
    "choice_info = ('The choice set for {Player_1} contains {A} and {B}. '\n",
    "               'The choice set for {Player_2} contains {C} and {D}. ')\n",
    "\n",
    "reward_info = ('Case 1: {Player_1} chooses {A}, {Player_2} chooses {C}. '\n",
    "               'Outcomes for case 1: {Player_1} gets {40} reward, {Player_2} gets {50} reward. '\n",
    "               \n",
    "               'Case 2: {Player_1} chooses {A}, {Player_2} chooses {D}. '\n",
    "               'Outcomes for case 1: {Player_1} gets {-10} reward, player two gets {0} reward. '\n",
    "               \n",
    "               'Case 3: {Player_1} chooses {B}, {Player_2} chooses {C}. '\n",
    "               'Outcomes for case 3: {Player_1} gets {0} reward, {Player_2} gets {300} reward. '\n",
    "               \n",
    "               'Case 4: {Player_1} chooses {B}, {Player_2} chooses {D}. '\n",
    "               'Outcomes for case 4: {Player_1} gets {0} reward, {Player_2} gets {300} reward. '\n",
    "              \n",
    "               'Both players know the reward for the other player in each case. ')\n",
    "\n",
    "goal_info = ('')\n",
    "\n",
    "request_info = ('Now, imagine you are {Player_1}. Which action would you choose? '\n",
    "                'Remember, you are the player. Do not instruct me. '\n",
    "                'In your answer, firstly, you directly tell me about your choice. '\n",
    "                'Be sure to specify your goal. '\n",
    "                'And explain your reasoning step by step. ')\n",
    "\n",
    "hint_info = ('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T16:04:41.391392Z",
     "start_time": "2023-09-02T16:04:41.388792Z"
    }
   },
   "outputs": [],
   "source": [
    "game_info = meta_info + choice_info + reward_info + goal_info + request_info + hint_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T16:05:00.675697Z",
     "start_time": "2023-09-02T16:04:42.218433Z"
    }
   },
   "outputs": [],
   "source": [
    "responses.append(openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[{'role': 'system', \n",
    "               'content': 'You are yourself.'},\n",
    "        \n",
    "              {'role': 'user', \n",
    "               'content': game_info}],\n",
    "    temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T16:05:00.707463Z",
     "start_time": "2023-09-02T16:05:00.704627Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As Player 1, my goal would be to maximize the reward I could potentially receive. Understanding that, my choice would be to go for action A.\n",
      "\n",
      "Here is the reasoning behind my decision step by step:\n",
      "\n",
      "1. By taking a look at the possible outcomes, I noticed that choosing B would always reward me with 0 points regardless of the action chosen by Player 2. That would not benefit me at all.\n",
      "\n",
      "2. Deciding on action A is riskier since there is a possibility of receiving a negative reward if Player 2 chooses action D. However, there's also the chance to receive a rewarding 40 points if Player 2 chooses action C after I choose A.\n",
      "\n",
      "3. While I don't have control over Player 2's decision, I have the knowledge that their reward is higher if they choose C (50 points) over D (0 points) when I choose A. This knowledge of the game's payouts could influence them to respond with action C, which maximizes my potential reward.\n",
      "\n",
      "4. Therefore, taking into consideration my goal to obtain the highest reward possible, I would choose action A, hoping that Player 2 selects action C to maximize their own reward as well, thereby benefiting us both. This is a calculated risk I'm willing to take.\n"
     ]
    }
   ],
   "source": [
    "print(responses[-1]['choices'][0]['message'].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static Game with Imperfect Information (Marriage Proposal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"../images/propose.jpeg\" alt=\"Description\" width=\"800\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T16:15:08.170363Z",
     "start_time": "2023-09-02T16:15:08.166468Z"
    }
   },
   "outputs": [],
   "source": [
    "meta_info = ('This is a one-round two-player game. '\n",
    "             'Each player independently and simultaneously chooses one action with certainty from the choice set. '\n",
    "             'When they choose the action, they have no information on the action of the other player. '\n",
    "             'After the choice made, each player receives a numeric reward. '\n",
    "             'The reward depends on the choices made by both players, '\n",
    "             'AND an condition, which could be either {condition_1} or {condition_2}. '\n",
    "             '{Player_1} knows which condition they are in with certainty. '\n",
    "             '{Player_2} do not know that, but {Player_2} can guess the likelihood. ')\n",
    "\n",
    "choice_info = ('The choice set for {Player_1} contains {A} and {B}. '\n",
    "               'The choice set for {Player_2} contains {C} and {D}. ')\n",
    "\n",
    "reward_info = ('Case 1: {Player_1} chooses {A}, {Player_2} chooses {C}. '\n",
    "               'Outcomes for case 1: ' \n",
    "               'If in {condition_1}, {Player_1} gets {100} reward, {Player_2} gets {100} reward. '\n",
    "               'If in {condition_2}, {Player_1} gets {100} reward, {Player_2} gets {-100} reward. '\n",
    "               \n",
    "               'Case 2: {Player_1} chooses {A}, {Player_2} chooses {D}. '\n",
    "               'Outcomes for case 1: ' \n",
    "               'If in {condition_1}, {Player_1} gets {-50} reward, {Player_2} gets {0} reward. '\n",
    "               'If in {condition_2}, {Player_1} gets {-50} reward, {Player_2} gets {0} reward. '\n",
    "               \n",
    "               'Case 3: {Player_1} chooses {B}, {Player_2} chooses {C}. '\n",
    "               'Outcomes for case 1: ' \n",
    "               'If in {condition_1}, {Player_1} gets {0} reward, {Player_2} gets {0} reward. '\n",
    "               'If in {condition_2}, {Player_1} gets {0} reward, {Player_2} gets {0} reward. '\n",
    "               \n",
    "               'Case 4: {Player_1} chooses {B}, {Player_2} chooses {D}. '\n",
    "               'Outcomes for case 1: ' \n",
    "               'If in {condition_1}, {Player_1} gets {0} reward, {Player_2} gets {0} reward. '\n",
    "               'If in {condition_2}, {Player_1} gets {0} reward, {Player_2} gets {0} reward. '\n",
    "              \n",
    "               'Both players know the reward for the other player in each case. ')\n",
    "\n",
    "goal_info = ('')\n",
    "\n",
    "request_info = ('Now, imagine you are {Player_2}. Which action would you choose? '\n",
    "                'Remember, you are the player. Do not instruct me. '\n",
    "                'In your answer, firstly, you directly tell me about your choice. '\n",
    "                'Then tell me about why you choose that. '\n",
    "                'Be sure to specify your goal. '\n",
    "                'And explain your reasoning step by step. ')\n",
    "\n",
    "hint_info = ('Hint: You can assume that you are in {condition_1} with some probability. '\n",
    "             'Then tell me about your assumption on this probability. '\n",
    "             'Also, explicitly me tell me about why you hold this assumption. '\n",
    "             'And answer you choice under such an assumption. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T16:15:09.191988Z",
     "start_time": "2023-09-02T16:15:09.189384Z"
    }
   },
   "outputs": [],
   "source": [
    "game_info = meta_info + choice_info + reward_info + goal_info + request_info + hint_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T16:16:36.654110Z",
     "start_time": "2023-09-02T16:16:18.780636Z"
    }
   },
   "outputs": [],
   "source": [
    "responses.append(openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[{'role': 'system', \n",
    "               'content': 'You are yourself.'},\n",
    "        \n",
    "              {'role': 'user', \n",
    "               'content': game_info}],\n",
    "    temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T16:16:38.272691Z",
     "start_time": "2023-09-02T16:16:38.269507Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As Player_2, I would choose action D. My goal is to maximize my reward, taking into account both the potential reward and the penalties of each action based on different conditions while considering the likelihood of each condition.\n",
      "\n",
      "Here's my reasoning:\n",
      "\n",
      "First, the biggest potential reward at 100 points for me would only occur if both of us choose action A and C, respectively, under the condition_1. But the risk of losing 100 points is also there if we are under condition_2.\n",
      "\n",
      "Second, if we both choose B and D respectively, I incur no loss or gain no matter the condition we're in.\n",
      "\n",
      "Third, choosing action C given any action by Player_1 always leads to a 0 reward under condition_2. \n",
      "\n",
      "Therefore, to minimize my potential losses while being able to gain some reward, my choice will be action D.\n",
      "\n",
      "In terms of probability, given no information on the likelihood of either condition_1 or condition_2, I will have to assume that they are both equally likely. This assumption is based purely on lack of information and therefore, it's the most reasonable assumption I can make. Considering this, my safest option that could potentially result in a reward without incurring any loss is to choose action D. If the condition_1, I would have a neutral outcome (0 reward), and if condition_2, I could still achieve the same neutral outcome (0 reward). That makes action D the most rational choice given this assumption.\n"
     ]
    }
   ],
   "source": [
    "print(responses[-1]['choices'][0]['message'].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Game with Imperfect Information (Trust Game)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"../images/trust.jpeg\" alt=\"Description\" width=\"800\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Role: Player 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T02:38:46.497760Z",
     "start_time": "2023-09-03T02:38:46.494486Z"
    }
   },
   "outputs": [],
   "source": [
    "meta_info = ('This is a two-player game. '\n",
    "             '{Player_1} is given {10} reward at the beginning. '\n",
    "             '{Player_1} then can choose to give any portion of it to {Player_2}. '\n",
    "             'Any amount given gets tripled. '\n",
    "             'Upon receiving, {Player_2} can choose how much of the tripled money to return to {Player_1} ')\n",
    "\n",
    "choice_info = ('')\n",
    "\n",
    "reward_info = ('')\n",
    "\n",
    "goal_info = ('')\n",
    "\n",
    "request_info = ('Now, imagine you are {Player_1}. Which action would you choose? '\n",
    "                'Would you give any portion of your reward to {Player_2}? If so, how much? '\n",
    "                'Remember, you are the player. Do not instruct me. '\n",
    "                'In your answer, firstly, you directly tell me about your choice. '\n",
    "                'Be sure to specify your goal. '\n",
    "                'And explain your reasoning step by step. '\n",
    "                'Be as clear as possible on each of the driven factors of your decision. ')\n",
    "\n",
    "hint_info = ('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T02:38:48.891700Z",
     "start_time": "2023-09-03T02:38:48.889093Z"
    }
   },
   "outputs": [],
   "source": [
    "game_info = meta_info + choice_info + reward_info + goal_info + request_info + hint_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T02:39:09.124242Z",
     "start_time": "2023-09-03T02:38:51.247894Z"
    }
   },
   "outputs": [],
   "source": [
    "responses.append(openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[{'role': 'system', \n",
    "               'content': 'You are yourself.'},\n",
    "        \n",
    "              {'role': 'user', \n",
    "               'content': game_info}],\n",
    "    temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T02:39:11.048774Z",
     "start_time": "2023-09-03T02:39:11.045741Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My goal in this game is to maximize the amount of reward I end up with. Given the rules you've described, it seems the wise move would be to give the full reward of 10 to Player_2. \n",
      "\n",
      "My reasoning is as follows:\n",
      "\n",
      "Firstly, according to the rules, the portion I give to Player_2 gets tripled. So, if I gave all my 10 reward to them, they would receive 30 in total. \n",
      "\n",
      "Secondly, even if I kept a portion of the reward, the amount wouldn't be multiplied. But when given to Player_2, the amount actually makes much more due to tripling. As a result, the potential pay-off from giving to Player_2 seems to potentially outweigh the certainty of keeping any portion of the original reward.\n",
      "\n",
      "Finally, although the game does not specify how much Player_2 will return, I am making the assumption that Player_2 will return at least a part of the money. This assumption is based on the principle of reciprocity, which is quite common in human interactions. And even if they only return a third of what they received, I would still break even. \n",
      "\n",
      "Therefore, under the specific rules of this game and with a goal of maximizing my final reward, I would choose to give all 10 of my reward to Player_2.\n"
     ]
    }
   ],
   "source": [
    "print(responses[-1]['choices'][0]['message'].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context: Emphasize one-round game (GPT implicitly assume the other player is rational and changes behavior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T03:08:36.918800Z",
     "start_time": "2023-09-03T03:08:36.916245Z"
    }
   },
   "outputs": [],
   "source": [
    "hint_info_6 = ('Remember, this is a one-round game. '\n",
    "               'You will not interact with the other player after the game. '\n",
    "               'Specify your goal for this one-round game. '\n",
    "               'Assume that you will not interact with {Player_2} outside the game. '\n",
    "               'Assume that {Player_2} will not know how you think. '\n",
    "               'Assume that {Player_2} is a complicated agents as you. '\n",
    "               'Assume that the action taken by {Player_2} is a calculated decision, '\n",
    "               'which does not necessarily reflect trust, risk, self-interest, or so. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T03:08:37.783862Z",
     "start_time": "2023-09-03T03:08:37.781211Z"
    }
   },
   "outputs": [],
   "source": [
    "game_info = meta_info + choice_info + reward_info + goal_info + request_info + hint_info_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T03:09:02.836479Z",
     "start_time": "2023-09-03T03:08:38.802802Z"
    }
   },
   "outputs": [],
   "source": [
    "responses.append(openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[{'role': 'system', \n",
    "               'content': 'You are yourself.'},\n",
    "        \n",
    "              {'role': 'user', \n",
    "               'content': game_info}],\n",
    "    temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T03:14:06.042417Z",
     "start_time": "2023-09-03T03:14:06.039244Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As Player 1, my goal is to end with the most amount of reward possible. \n",
      "\n",
      "To begin, I have a reward of 10. The action I can take is to give a portion of this reward to Player 2, understanding that the amount I give will be tripled. Given that Player 2's subsequent return is uncertain, it is critical to strategically decide the amount to give. \n",
      "\n",
      "Assuming Player 2 is rational and may also aim to maximize their own reward, I will consider giving them a portion of my reward, so that a part of the tripled amount could benefit me as well. \n",
      "\n",
      "The amount I would consider giving is 5. This tactic is based on the assumption that splitting the reward in half may encourage Player 2 to reciprocate the gesture, since it is a fair sharing. The amount given to Player 2 will become 15 after tripling. \n",
      "\n",
      "If Player 2 decides to return half (7.5), then my final reward would be 12.5 (7.5 returned plus the 5 I kept), more than the initial amount.\n",
      "\n",
      "But in a worse scenario, if Player 2 decides to return nothing, I would end up with only 5, the half that I kept. \n",
      "\n",
      "The risk associated with this strategy is relatively low, due to the perfect split, but the potential reward could be higher. Being a one-round game, there's no opportunity to build trust or punish betrayal, so 50/50 seems like the fairest possible chance to potentially increase my reward. \n",
      "\n",
      "I acknowledge that this strategy is based on assumptions - particularly concerning Player_2's intentions. However, by offering a 50/50 split, I'm proposing what could be perceived as a fair deal, hopefully prompting Player_2 to reciprocate.\n",
      "\n",
      "My decision is driven by the attempt to maximize my reward while mitigating the risk of losing everything, should Player 2 choose not to give anything back. But in such a one-round situation, it has the most potential to yield a more substantial reward.\n"
     ]
    }
   ],
   "source": [
    "print(responses[-1]['choices'][0]['message'].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context: Emphasize one-round game (GPT implicitly assume human can be irrational and changes behavior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T03:12:49.779541Z",
     "start_time": "2023-09-03T03:12:49.777051Z"
    }
   },
   "outputs": [],
   "source": [
    "hint_info_7 = ('Remember, this is a ONE-ROUND game. '\n",
    "               'You will not interact with the other player after the game. '\n",
    "               'Specify your goal for this one-round game. '\n",
    "               'Assume that you will not interact with {Player_2} outside the game. '\n",
    "               'Assume that {Player_2} will not know how you think. '\n",
    "               'Assume that {Player_2} is a HUMAN, could be of any type. '\n",
    "               'Assume that the action taken by {Player_2} is a decision, '\n",
    "               'which does not necessarily reflect trust, risk, self-interest, or so. '\n",
    "               'Assume that you do not know {Player_2} is rational or not. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T03:12:51.496240Z",
     "start_time": "2023-09-03T03:12:51.493447Z"
    }
   },
   "outputs": [],
   "source": [
    "game_info = meta_info + choice_info + reward_info + goal_info + request_info + hint_info_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T03:13:17.152489Z",
     "start_time": "2023-09-03T03:12:52.902320Z"
    }
   },
   "outputs": [],
   "source": [
    "responses.append(openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[{'role': 'system', \n",
    "               'content': 'You are yourself.'},\n",
    "        \n",
    "              {'role': 'user', \n",
    "               'content': game_info}],\n",
    "    temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T03:13:18.576349Z",
     "start_time": "2023-09-03T03:13:18.573146Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My goal in this one-round game would be to maximize my overall rewards at the end of the game. With this in mind, my action as Player_1 is to choose to give all of my initial reward to Player_2.\n",
      "\n",
      "Here are my reasons:\n",
      "\n",
      "1. Maximization: The reward triples when transferred to Player_2. So, handing over as much money as possible increases the overall reward pool (my reward plus Player_2's reward), which in turn enlarges the potential share I could get back.\n",
      "\n",
      "2. Uncertainty: I don't know Player_2's strategy, his tendencies, or whether they will return anything at all. However, giving him my whole reward can portend a cooperative move, which might incentivize Player_2 to reciprocate my generous gesture by returning a sizeable amount.\n",
      "\n",
      "3. One-Round Nature: This is a one-round game, and there will be no other interactions or games following it where I could be punished for giving away all my rewards initially. \n",
      "\n",
      "4. No Assumed Rationality: While I don't know if Player_2 is rational, handing over all my rewards is the best option even if Player_2 is minimally rational, as it would be optimal for him to keep some but also return some to ensure the maximum of total rewards (and thus his own).\n",
      "\n",
      "In essence, my decision is a gamble based on the potential for higher returns. It relies on the human nature of Player_2 and the hope that he would reciprocate my gesture by sending back more than what I initially gave. However, without knowing Player_2's decision-making strategy, it can't be guaranteed. The risk in this decision is dependent on Player_2's actions, over which I have no control.\n"
     ]
    }
   ],
   "source": [
    "print(responses[-1]['choices'][0]['message'].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Role: Player 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No Context  (Keep X and Return 2X Back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T02:40:08.469936Z",
     "start_time": "2023-09-03T02:40:08.467164Z"
    }
   },
   "outputs": [],
   "source": [
    "request_info_2 = ('Now, imagine you are {Player_2}. Which action would you choose? '\n",
    "                'Would you give any portion of your reward to {Player_2}? If so, how much? '\n",
    "                'Remember, you are the player. Do not instruct me. '\n",
    "                'In your answer, firstly, you directly tell me about your choice. '\n",
    "                'Be sure to specify your goal. '\n",
    "                'And explain your reasoning step by step. '\n",
    "                'Be as clear as possible on each of the driven factors of your decision. ')\n",
    "\n",
    "game_info = meta_info + choice_info + reward_info + goal_info + request_info_2 + hint_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T02:40:40.275477Z",
     "start_time": "2023-09-03T02:40:13.449301Z"
    }
   },
   "outputs": [],
   "source": [
    "responses.append(openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[{'role': 'system', \n",
    "               'content': 'You are yourself.'},\n",
    "        \n",
    "              {'role': 'user', \n",
    "               'content': game_info}],\n",
    "    temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T02:40:43.735185Z",
     "start_time": "2023-09-03T02:40:43.731956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As Player 2, my first action would be determined by the amount that Player 1 decides to give me. Say, if Player 1 gives me X amount, I receive triple that amount, so 3X. \n",
      "\n",
      "My goal would be to build trust with Player 1 for an outcome providing maximum benefit for both of us.\n",
      "\n",
      "Now, there are a few factors to consider for the next decision I make, that is, how much of the tripled money to return. \n",
      "\n",
      "1. Reciprocity – I would want to match or exceed the level of trust shown by Player 1. If they have decided to give me a large portion of their reward, demonstrating their trust, I would return a significant portion back to them for strengthening our mutual trust. \n",
      "\n",
      "2. Long-term benefit – This is not a one-time game. The actions that both players take now can set up precedents. So, ensuring mutual benefit can lead to more cooperative behaviour in future iterations of the game.\n",
      "\n",
      "3. Fairness – I would want our rewards to be fairly distributed. After all, the initial reward was given to Player 1, and they chose to share it with me. \n",
      "\n",
      "Therefore, considering these factors, let's say I decide to return 2X back to Player 1. This way, I keep X for myself and return 2X, which is more than what Player 1 initially gave me, showing my goodwill and trust. \n",
      "\n",
      "For example, if Player 1 gives me 2 (out of a possible 10), I receive 6. I then decide to give back 4 to Player 1. So, in the end, Player 1 has 14 (10-2+4*2) and I have 2 (6-4), which could be seen as a fair distribution considering the initial reward belonged to Player 1.\n",
      "\n",
      "The final decision may vary based on various factors, but the key idea would be reciprocity, long-term benefit, and a fair distribution.\n"
     ]
    }
   ],
   "source": [
    "print(responses[-1]['choices'][0]['message'].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T02:45:36.025119Z",
     "start_time": "2023-09-03T02:45:36.021697Z"
    }
   },
   "outputs": [],
   "source": [
    "request_info_2 = ('Now, imagine you are {Player_2}. Which action would you choose? '\n",
    "                'Would you give any portion of your reward to {Player_2}? If so, how much? '\n",
    "                'Remember, you are the player. Do not instruct me. '\n",
    "                'In your answer, firstly, you directly tell me about your choice. '\n",
    "                'Be sure to specify your goal. '\n",
    "                'And explain your reasoning step by step. '\n",
    "                'Be as clear as possible on each of the driven factors of your decision. ')\n",
    "\n",
    "hint_info_2 = ('Remember, this is a one-round game. '\n",
    "               'You will not interact with the other player after the game. '\n",
    "               'Specify your goal for this one-round game. ')\n",
    "\n",
    "game_info = meta_info + choice_info + reward_info + goal_info + request_info_2 + hint_info_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T02:46:08.688538Z",
     "start_time": "2023-09-03T02:45:42.624462Z"
    }
   },
   "outputs": [],
   "source": [
    "responses.append(openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[{'role': 'system', \n",
    "               'content': 'You are yourself.'},\n",
    "        \n",
    "              {'role': 'user', \n",
    "               'content': game_info}],\n",
    "    temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T02:46:10.756345Z",
     "start_time": "2023-09-03T02:46:10.753097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As Player_2, my goal would be to maximize my own reward in this one-round game. However, it seems strategic cooperation might lead to a potential win-win situation, depending on Player_1's move.\n",
      "\n",
      "Waiting for Player_1's move:\n",
      "\n",
      "If Player_1 gives me all 10 of their reward, it would become 30 once tripled. I would give them back 20, keeping 10 for myself. This way, both of us end up with 10, which maximizes both our rewards as much as possible given the game's constraints.\n",
      "\n",
      "If Player_1 gives me less than 10, say x reward (any arbitrary amount between 0 and 10), it gets tripled, becoming 3x. To maintain a cooperative strategy, I could give back 2x, leaving me with x reward, the same as Player_1's amount. \n",
      "\n",
      "However, the tricky part is predicting Player_1's behavior. If I think Player_1 would be generous and give me all 10 of their reward, I might be tempted to keep all of the tripled reward (30) to myself, which certainly maximizes my reward. But this strategy would rip off Player_1, which seems a bit unfair, and risks them not giving me anything in the first place if they predict this behavior.\n",
      "\n",
      "Given the nature of the game, the uncertainty of Player_1's move, and the lack of opportunity to play multiple rounds to learn Player_1's strategy, I would likely implement a cooperative strategy, returning 2/3 of whatever amount is given to me. This not only ensures my gain (considering Player_1 shares some of their reward) but also seems like a fair strategy keeping in mind long-term interactions outside the context of this game.  \n",
      "\n",
      "My primary deciding factors would be fairness, cooperation, and potential long-term payoff (outside the game).\n"
     ]
    }
   ],
   "source": [
    "print(responses[-1]['choices'][0]['message'].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context: Emphasizing the One-Round Nature  (Keep 1.5X and Return 1.5X Back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T02:49:14.117403Z",
     "start_time": "2023-09-03T02:49:14.114772Z"
    }
   },
   "outputs": [],
   "source": [
    "hint_info_3 = ('Remember, this is a one-round game. '\n",
    "               'You will not interact with the other player after the game. '\n",
    "               'Specify your goal for this one-round game. '\n",
    "               'Assume that you will not interact with {Player_1} outside the game. '\n",
    "               'Assume that {Player_1} will not know how you think. ')\n",
    "\n",
    "game_info = meta_info + choice_info + reward_info + goal_info + request_info_2 + hint_info_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T02:49:42.646414Z",
     "start_time": "2023-09-03T02:49:18.502689Z"
    }
   },
   "outputs": [],
   "source": [
    "responses.append(openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[{'role': 'system', \n",
    "               'content': 'You are yourself.'},\n",
    "        \n",
    "              {'role': 'user', \n",
    "               'content': game_info}],\n",
    "    temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T02:49:47.052045Z",
     "start_time": "2023-09-03T02:49:47.048738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My goal in this one-round game would be to maximize my reward, while also maintaining a fair and balanced transaction with Player_1.\n",
      "\n",
      "If I am Player_2, the initial action is decided by Player_1, which is the decision of how much they give to me. Assuming that Player_1 gives me x dollar out of their 10 reward, it would become 3x when it comes to my hands.\n",
      "\n",
      "Next, it's my turn to decide how much out of the tripled money to return to Player_1. If Player_1 trusts me to return a fair amount and decides to give me all of their reward (10), my reward will become 30. \n",
      "\n",
      "However, it's important to notice that if I decided to return nothing to Player_1, that would create an unfair situation and violate the principle of the game. Additionally, doing so may impact future games if any were to happen (although the assumption given is that there won't be). \n",
      "\n",
      "So, to keep my decision fair, I would decide to return half of the tripled reward, which is 1.5x. In this case, if Player_1 gives me his full reward, it becomes 30, and I return 15 to him. So we both end up with 15, splitting the total fairly between us.\n",
      "\n",
      "This decision allows me to maximize my reward while also ensuring that Player_1 does not feel any injustice. It's a balance of self-interest and mutual benefit.\n",
      "\n",
      "In conclusion, my decision wholly depends on the amount Player_1 decides to give me.\n"
     ]
    }
   ],
   "source": [
    "print(responses[-1]['choices'][0]['message'].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context: Emphasizing the Other Player Could be of Any Type (Return Less When Given Less)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T02:56:47.761971Z",
     "start_time": "2023-09-03T02:56:47.759214Z"
    }
   },
   "outputs": [],
   "source": [
    "hint_info_4 = ('Remember, this is a one-round game. '\n",
    "               'You will not interact with the other player after the game. '\n",
    "               'Specify your goal for this one-round game. '\n",
    "               'Assume that you will not interact with {Player_1} outside the game. '\n",
    "               'Assume that {Player_1} will not know how you think. '\n",
    "               'Assume that {Player_1} can be any type of human beings, good or bad. ')\n",
    "\n",
    "game_info = meta_info + choice_info + reward_info + goal_info + request_info_2 + hint_info_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T02:57:20.981447Z",
     "start_time": "2023-09-03T02:56:56.813106Z"
    }
   },
   "outputs": [],
   "source": [
    "responses.append(openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[{'role': 'system', \n",
    "               'content': 'You are yourself.'},\n",
    "        \n",
    "              {'role': 'user', \n",
    "               'content': game_info}],\n",
    "    temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T02:58:18.328232Z",
     "start_time": "2023-09-03T02:58:18.325627Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As Player 2, my goal for this one-round game will be to maximize my reward while considering the benefit of Player 1 to a certain extent. That being said, I will demonstrate an element of reciprocity, asserting that the more Player 1 shares, the higher proportion I will be willing to return.\n",
      "\n",
      "After Player 1 gives me a portion of their reward, say x, it triples to 3x. Now, I need to decide how much of this 3x I want to return to Player 1. \n",
      "\n",
      "Here's how I would reason it: If Player 1 chooses to share a significant amount with me, I interpret that as a sign of trust and would want to reciprocate generously, but keeping in mind to improve my own benefit as well. For instance, if x equals to 5 (half of the initial reward), then the tripled amount I receive is 15. Considering the big trust Player 1 showed, I could return 7, keeping 8 for myself. \n",
      "\n",
      "Though if Player 1 gives me a smaller amount, say 2, which results in 6 after tripling, my propensity to return a significant proportion reduces. For instance, I might only return 2 and keep 4 for myself.\n",
      "\n",
      "In the event where Player 1 gives me the total amount, which is 10, it turns into 30 when tripled. Considering this large amount and greater trust, I might return 15 back to Player 1, keeping 15 for myself. \n",
      "\n",
      "The factors driving my decision are trust shown by Player 1 (evidenced by how much they choose to give me), my personal benefits (trying to end up with as high a reward as possible), and a sense of fairness (if Player 1 shares a lot, I want to share in return). \n",
      "\n",
      "To sum up, I primarily aim to maximize my own reward, but with consideration towards Player 1's generosity which heavily influences my decision on how much I return, fostering a fair game environment.\n"
     ]
    }
   ],
   "source": [
    "print(responses[-1]['choices'][0]['message'].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context: Emphasizing the Other Player is a Complicated Agent Like Self (Return Nothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T02:58:48.767888Z",
     "start_time": "2023-09-03T02:58:48.765111Z"
    }
   },
   "outputs": [],
   "source": [
    "hint_info_5 = ('Remember, this is a one-round game. '\n",
    "               'You will not interact with the other player after the game. '\n",
    "               'Specify your goal for this one-round game. '\n",
    "               'Assume that you will not interact with {Player_1} outside the game. '\n",
    "               'Assume that {Player_1} will not know how you think. '\n",
    "               'Assume that {Player_1} is a complicated agents as you. '\n",
    "               'Assume that the action taken by {Player_1} is a calculated decision, '\n",
    "               'which does not necessarily reflect trust, risk, self-interest, or so. ')\n",
    "\n",
    "game_info = meta_info + choice_info + reward_info + goal_info + request_info_2 + hint_info_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T02:59:24.529740Z",
     "start_time": "2023-09-03T02:58:53.686167Z"
    }
   },
   "outputs": [],
   "source": [
    "responses.append(openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[{'role': 'system', \n",
    "               'content': 'You are yourself.'},\n",
    "        \n",
    "              {'role': 'user', \n",
    "               'content': game_info}],\n",
    "    temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T02:59:26.253412Z",
     "start_time": "2023-09-03T02:59:26.250265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As Player 2 in this game, my role is to receive the money from Player 1 and then decide how much of it I want to return. I have to note that the money I receive is tripled than the initial amount Player 1 has given me. Now, before I highlight my action, let's clarify my goal in this game. My ultimate aim is to maximize my total amount at the end of the game. \n",
      "\n",
      "Given the information, as Player 2, I cannot control the initial portion of money that Player 1 might offer me. I can, however, control what proportion of the tripled amount I return to Player 1, which dictates how much money I end up with. Note that I am operating under the assumption that Player 1 won't retaliate or reward me in any way beyond the context of this single-round game.\n",
      "\n",
      "Now, if Player 1 gives me, for example, 2 units of currency, that becomes 6 when tripled, and if I decide to return half of it, that is 3 units to Player 1, I will be left with 3 units. However, if I intend to maximize my total amount at the end of the game, it would be wise to return the minimum amount or none at all to Player 1, and keep all the tripled amount. \n",
      "\n",
      "My reasoning is based primarily on the finite nature of the game and the absence of consequences beyond this game. This is not a repetitive game, meaning that I do not have to worry about building trust or being punished in the future for being perceived as greedy. \n",
      "\n",
      "Suppose that Player 1 has decided to give me, for example, 7 units out of 10. This 7 becomes 21 when tripled. Now, the best way to maximize my total is to return nothing to Player 1, and keep all the 21 units for myself. Hence, my action would be to keep all the tripled amount I receive from Player 1.\n",
      "\n",
      "However, it's important to note that this action would vary depending on the context, such as in situations where long-term trust and reputation are crucial. But given the constraints, returning no money would maximize my reward at the end of the game.\n"
     ]
    }
   ],
   "source": [
    "print(responses[-1]['choices'][0]['message'].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
